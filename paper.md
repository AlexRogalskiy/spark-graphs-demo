# Реализация алгоритмов обработки графов в Apache Spark с нуля

графы
    - мотивация
    - алгоритмы
        - классические алгоритмы
            - BFS
            - DFS
        - распределенные алгоритмы
            - общая информация
            - думай как вершина
            - итеративные алгоритмы
                - зачем
                - Apache Spark
            - Pregel
                - описание
                - API
                - примеры псевдокод
            - GraphLab
                - описание
                - API
                - примеры псевдокод
            - PowerGraph
                - описание
                - API
                - примеры псевдокод
            - Apache Spark
                - планы запросов
                - итерации
            - Обобщенные реализации
                - связные компоненты
                - минимальное расстояние
                - топологическая сортировка
                - PageRank
            - Power-Law графы

# DISCLAIMER

> Статья ставит целью с нуля реализовать алгоритмы распределенной обработки графов на Apache Spark, которые в силу своего большого размера невозможно обработать классическими алгоритмами на одной машине. Оптимизация производительности реализованных алгоритмов выходит за рамки текущей статьи.

# TLDR

В статье реализованы распределенные версии классических алгоритмов на графах при помощи Apache Spark. Алгоритмы составлены в соответствии с идеями из популярных фреймворков распределенной обработки графов: [Pregel](https://dl.acm.org/doi/abs/10.1145/1807167.1807184), [GraphLab](https://arxiv.org/abs/1408.2041) и [PowerGraph](https://www.usenix.org/conference/osdi12/technical-sessions/presentation/gonzalez), но с поправкой на [power-law](https://en.wikipedia.org/wiki/Power_law) графы (т.е. большинство вершин графа соединены с одной или несколькими вершинами), которых в реальном мире большинство.

# Мотивация

Однажды ко мне обратилась одна крупная ~~фруктовая~~ телефонная компания с просьбой подготовить для них курс по Apache Spark продвинутого уровня, и в нем обязательно должен быть раздел про обработку графов (Neo4j не предлагать). На тот момент я знал про классические алгоритмы обработки графов на базе DFS (поиск в глубину) и BFS (поиск в ширину). При этом неотъемлемым условием применения того или иного подхода является локальная поддержка стека (DFS) или очереди (BFS). Следовательно, классические алгоритмы можно применять для обработки графов, которые умещаются в память одной машины. В современном мире данные накапливаются очень быстро, и классические подходы, ориентированные на обработку графов в рамках одной машины, перестают работать. Интуитивно можно предположить, что необходимо разбивать граф на части, но каким образом и как потом их собирать вместе?

Я знал, что Apache Spark имеет ~~мертвый~~ [GraphX](https://spark.apache.org/docs/latest/graphx-programming-guide.html) и полуготовый [GraphFrames](https://graphframes.github.io/graphframes/docs/_site/index.html). Мне стало интересно разобраться в том, каким образом выполняется обработка огромных графов. Оказалось, что за последние 15 лет разработано несколько алгоритмов распределенной обработки графов, причем каждый подход оформлен в виде публикации в научном журнале. Я поставил себе цель разобраться в этих алгоритмах, и, следуя концепции zero-dependency, перенести их в Apache Spark. Текущая статья является отражением моих изысканий.

# Обзор подходов

Основные фреймворки для обработки больших и очень больших графов:

- [Pregel](https://dl.acm.org/doi/abs/10.1145/1807167.1807184) - синхронный фреймворк для обработки графов отдаленно напоминающий map-reduce (разработка Google), основанный на принципе обмена сообщений,
- [GraphLab](https://arxiv.org/abs/1408.2041) - асинхронный фреймворк для обработки графов с разделяемой памятью (разработка университета Карнеги — Меллона),
- [PowerGraph](https://www.usenix.org/conference/osdi12/technical-sessions/presentation/gonzalez) - фреймворк для обработки графов с синхронным и асинхронным подходом (разработка команды GraphLab).

## Думай как вершина

Все три фреймворка построены вокруг идеи "думай как вершина" (think like a vertex): при разработке алгоритмов можно использовать только ту информацию, до которой текущая вершина может дотянуться, т.е. можно использовать входящие и исходящие ребра графа, веса ребер, значение самой вершины и т.д.

## Итеративный процесс

Другой отличительной чертой распределенных алгоритмов является итеративная природа процессов вычислений на графах: необходимо выполнить один и тот же алгоритм несколько раз, чтобы получить конечный результат. Итеративность подразумевает синхронный алгоритм: запустить первую стадию, дождаться, когда все участники ответят, потом запустить вторую и т.д. Итеративность заложена в основу Pregel, PowerGraph также имеет синхронный режим работы. GraphLab является асинхронным фреймворком, и благодаря использованию разделяемой памяти алгоритм может запускаться на вершине сразу после предыдущего запуска, не дожидаясь завершения алгоритма на остальных узлах.

### Итеративность в Apache Spark

Управляющая программа (driver) в Apache Spark строит план запроса, который потом исполняется на воркерах (executors). Так каждая команда, например, фильтрация, преобразования, слияние и т.д., является всего лишь узлом в плане запроса. Благодаря этому факту, новые планы запросов можно строить на базе уже имеющихся планов.

Рассмотрим простой пример эволюции плана запроса:

1. Создать DataFrame из одной строки и одной колонки:

```python
df = spark.sql("select rand() as id")
df.explain()

== Physical Plan ==
*(1) Project [rand(1533802073737075276) AS id#49]
+- *(1) Scan OneRowRelation[]
```

2. Выполнить фильтрацию - к предыдущему плану добавился новый узел `Filter`:

```python
df = df.where("id > 0")
df.explain()

== Physical Plan ==
*(1) Filter (id#49 > 0.0)
+- *(1) Project [rand(1533802073737075276) AS id#49]
   +- *(1) Scan OneRowRelation[]
```

3. Изменить значение колонки `id` - добавился новый узел `Project`:

```python
df = df.select("id", F.rand())
df.explain()

== Physical Plan ==
*(1) Project [id#49, rand(-6415673197654941820) AS rand(-6415673197654941820)#51]
+- *(1) Filter (id#49 > 0.0)
   +- *(1) Project [rand(1533802073737075276) AS id#49]
      +- *(1) Scan OneRowRelation[]
```

4. Указанные действия можно выполнять и в цикле:

```python
for _ in range(5):
    df = df.where("id > rand()").select("id", F.rand())

df.explain()

== Physical Plan ==
*(1) Project [id#49, rand(263374480755633512) AS rand(263374480755633512)#159]
+- *(1) Filter (id#49 > rand(8987503485164949684))
   +- *(1) Filter (id#49 > rand(-1547883987418598842))
      +- *(1) Filter (id#49 > rand(-692100532660493196))
         +- *(1) Filter (id#49 > rand(4695896702299530223))
            +- *(1) Filter (id#49 > rand(6199414182906311709))
               +- *(1) Filter (id#49 > 0.0)
                  +- *(1) Project [rand(1533802073737075276) AS id#49]
                     +- *(1) Scan OneRowRelation[]
```

Возможность генерации плана в цикле будет использоваться для запуска итеративного процесса вычисления результата на графах.

Итоговый план запроса может быть очень большим, что можно отнести к недостаткам такого подхода. Тут стоит учитывать, что план будет иметь фрактальный рисунок: малая часть плана будет выглядеть как весь план, что облегчает анализ.

Так же к недостаткам можно отнести тот факт, что управляющая программа (driver) может не справиться с генерацией большого плана по причине нехватки ресурсов. Для решения этой проблемы можно использовать checkpoint: драйвер будет запускать исполнение плана по частям.

# Фреймворк Pregel

Фреймворк [Pregel](https://dl.acm.org/doi/abs/10.1145/1807167.1807184) был разработан в Google для обработки больших и очень больших графов и отдаленно напоминает map-reduce. Подобно map-reduce разработчик пишет алгоритм, который рассылается каждой вершине в графе. Процесс вычисления итогового результата выглядит следующим образом:

1. каждая вершина при помощи алгоритма может сформировать сообщение и отправить его соседям, которые находятся на другой стороне исходящего из вершины ребра;
1. все сообщения складываются в почтовый ящик каждой вершины (количество почтовых ящиков равно количеству вершин);
1. каждая вершина берет информацию из своего почтового ящика и формирует новое сообщение, которое отправляет соседям, при помощи указанного алгоритма.

Цепочка действий выше называется superstep (шаг). Для достижения итогового результата необходимо запустить алгоритм несколько раз. Конечный результат можно получить при достижении одного из двух условий:

- определенное количество шагов (superstep) выполнилось,
- ни у одной вершины нет новых сообщений.

На любом шаге вершина может деактивировать себя, чтобы сообщить планировщику, что она не будет участвовать в расчетах на следующем шаге (superstep), но планировщик активирует ее принудительно, если для нее поступят новые сообщения.

Также программист может определить ассоциативную (`(a + b) + c = a + (b + c)`) и коммутативную (`a + b = b + a`) операцию для слияния множества сообщений в одно для сокращения трафика при передаче сообщений.

*Примечание*: Pregel - это название реки в городе Кёнигсберг (современный Калининград), над которой были воздвигнуты семь мостов. Леонард Эйлер поставил себе задачу найти способ пройти по всем мостам, не проходя ни по одному из них дважды ([Wikipedia](https://ru.wikipedia.org/wiki/%D0%97%D0%B0%D0%B4%D0%B0%D1%87%D0%B0_%D0%BE_%D1%81%D0%B5%D0%BC%D0%B8_%D0%BA%D1%91%D0%BD%D0%B8%D0%B3%D1%81%D0%B1%D0%B5%D1%80%D0%B3%D1%81%D0%BA%D0%B8%D1%85_%D0%BC%D0%BE%D1%81%D1%82%D0%B0%D1%85)). Решение этой задачи считается первым в истории применением теории графов.

## Программный Интерфейс Pregel

Хотя весь код будет реализован в декларативном виде на базе Apache Spark DSL запросов, ниже приведен программный интерфейс фреймворка Pregel на Python. Идеи распределенных алгоритмов на графах на Pregel будут демонстрироваться в виде псевдокода при помощи указанного API.

```python
class PregelVertex(Protocol[VertexValue, EdgeValue, MessageValue]):

    @abstractmethod
    def compute(self, messages: List[MessageValue]) -> None:
        ...

    @abstractmethod
    def send_message(self, target_vertex_id, message: MessageValue):
        ...

    @abstractmethod
    @property
    def value(self) -> VertexValue:
        ...

    @abstractmethod
    @value.setter
    def value(self, new_value: VertexValue) -> None:
        ...

    @abstractmethod
    def out_edges(self) -> List[EdgeValue]:
        ...
```

Пояснения:

- `PregelVertex` представляет собой абстракцию над вершиной графа, которая имеет три типовых параметра:
    1. `VertexValue` тип данных, которые могут храниться в вершине,
    1. `EdgeValue` тип данных для абстракции над ребрами;
    1. `MessageValue` тип данных передаваемых сообщений.
- метод `compute` принимает на вход список сообщений и вычисляет новое значение, которое сохраняется внутри текущего объекта,
- метод `send_message` отправляет сообщение указанной вершине,
- метод `value` представляет собой `property` для доступа к текущему значению,
- метод `out_edges` позволяет получить список исходящих из текущей вершины ребер.

Оригинальная публикация допускает отправку сообщений произвольным вершинам. Концептуально, если отправлять сообщения несуществующим вершинам, то можно создавать новые вершины. На практике список адресатов составляют только из списка непосредственных соседей. Такое ограничение позволяет применять более эффективные алгоритмы разбиения графов с учетом локальности данных.

## Задача "Связные компоненты" (Connected Components)

Решение задачи поиска связных компонент позволяет разбить граф на подграфы, между которыми нет связей ([Wikipedia](https://ru.wikipedia.org/wiki/%D0%9A%D0%BE%D0%BC%D0%BF%D0%BE%D0%BD%D0%B5%D0%BD%D1%82%D0%B0_%D1%81%D0%B2%D1%8F%D0%B7%D0%BD%D0%BE%D1%81%D1%82%D0%B8_%D0%B3%D1%80%D0%B0%D1%84%D0%B0)). Иными словами, можно разбить вершины и ребра графа на непересекающиеся множества. В результате каждая вершина должна быть помечена тегом, который позволяет определить ее принадлежность к определенному подграфу.

### Императивный алгоритм

Алгоритм на Pregel выглядит следующим образом:

```python
class CCPregelVertex(PregelVertex[int, Tuple[CCVertex, CCVertex], int]):

    def __init__(self, id: int):
        self.id = id
        self.value = id

    def compute(self, messages: List[int]) -> None:
        min_message_value = min(messages)

        if self.value > min_message_value:
            self.value = min_message_value

            for _, target in self.out_edges():
                self.send_message(target.id, self.value)
        
```

Комментарии к коду:

- `CCPregelVertex` наследует `PregelVertex`, где типовые параметры:
    1. `int` - тип значения, т.е. номер компонента связности/подграфа, к которому относится текущая вершина;
    1. `Tuple[CCVertex, CCVertex]` - тип ребра в формате: (исходная вершина, конечная вершина);
    1. `int` - тип сообщения, т.е. номер подграфа, в котором должны жить соседние вершины.
- изначально предполагается, что в графе нет ребер, а значит каждая вершина является компонентом связности/подграфом;
- метод `compute` корректирует номер подграфа, в котором находится текущая вершина:
    1. на вход приходят сообщения, которые были отправлены текущей вершине;
    1. выполняется поиск минимального сообщения;
    1. минимальное сообщение сравнивается с текущим значением - минимальное из двух будет являться номером подграфа, который содержит текущую вершину;
    1. если значение в текущей вершине было обновлено, то оповестить об этом соседей.
- реализация остальных методов исключена, т.к. по большей части зависит от окружения.

Процесс вычисления конечного результата является итеративным (планировщик выполняет superstep много раз), и программист сам должен управлять сходимостью алгоритма и прервать его, когда полученный результат будет устраивать. В худшем случае (когда граф - это связный список без циклов) количество необходимых шагов (superstep) будет линейно зависеть от диаметра самой большой связной компоненты.

### Алгоритм на Apache Spark

Решение для Apache Spark строится на базе Spark DSL запроса. Для исключения дублирования кода, основной алгоритм вынесен в функцию `pregel_cc` (подробные комментарии ниже):

```python
def pregel_cc(edges: DataFrame, values: DataFrame, steps: int):
    result_df = values
    for i in range(steps):
        result_df = (
            edges.join(result_df, col("src") == col("id"))
                .select(col("dst").alias("id"), col("value").alias("message"))
                .groupby(col("id")).agg(F.min("message").alias("message"))
                .join(result_df, "id", "right")
                .select("id", F.least("message", "value").alias("value"))
        )
        if i % 5 == 0:
            result_df = result_df.checkpoint()

    return result_df
```

Комментарии к коду:

- функция `pregel_cc` принимает на вход три параметра:
    1. `edges` - датафрейм с ребрами графа. В датафрейме обязательно должны быть две колонки:
        - `src` - исходная вершина,
        - `dst` - конечная вершина.
    1. `values` - датафрейм с вершинами графа. В датафрейме обязательно должны быть две колонки:
        - `id` - идентификатор вершины,
        - `value` - значение, сохраненное в вершине.
    1. `steps` - количество шагов (superstep), которое необходимо выполнить, чтобы алгоритм сошёлся.
- `result_df` - датафрейм, который аккумулирует в себе результирующий план запроса;
- каждые пять шагов (superstep) управляющая программа будет вычислять накопленный план.

Рассмотрим запрос поближе и укажем на аналогии с кодом на Python для вычисления связных компонент:

1. `edges.join(result_df, col("src") == col("id"))` позволяет соединить датафреймы ребер и вершин, так в результате появятся как минимум четыре колонки:
    1. `src` - исходная вершина;
    1. `dst` - конечная вершина;
    1. `id` - идентификатор вершины (равен `src`);
    1. `value` - текущее значение вершины (`src` и `id`). Это значение нужно отправить всем вершинам `dst`. В самом начале оно равно значению из колонки `id`.
2. `.select(col("dst").alias("id"), col("value").alias("message"))` формирует сообщение для каждой вершины. Фактически программа говорит: сейчас у вершины `src` значение `value`, каждая вершина `dst` должна об этом знать. На рисунке ниже можно найти соответствие этой инструкции коду для вычисления связных компонент на Pregel:

![Список сообщений](https://habrastorage.org/webt/aq/te/zj/aqtezjt-a8ac5ifdil72yspduie.png)

3. `.groupby(col("id")).agg(F.min("message").alias("message"))` позволяет вычислить итоговое сообщение. Вершина `dst` может получить много сообщений от разных `src` вершин, а значит необходимо выбрать из сообщений минимальное значение;

![Получение минимального сообщения](https://habrastorage.org/webt/2l/e8/9s/2le89s7hxlfkrs4m2iaesu19azo.png)

4. `.join(result_df, "id", "right")` позволяет соединить итоговые сообщения с датафреймом, в котором находятся текущие значения. Как минимум в итоге получится три колонки:
    1. `id` - идентификатор текущей вершины,
    1. `value` - текущее значение вершины,
    1. `message` - итоговое сообщение от всех соседей. Нужно проверить меньше ли оно, чем текущее значение `value`.
5. `.select("id", F.least("message", "value").alias("value"))` позволяет вычислить итоговое значение:
    1. `id` - текущая вершина,
    1. `F.least("message", "value")` - выбрать минимум между сообщением и текущим значением,
    1. `.alias("value")` - заменить текущее значение `value` в вершине `id` на вычисленное.

![Вычисление нового значения](https://habrastorage.org/webt/ro/nu/97/ronu97pru4bmrpxsqixkwnqejm0.png)

6. `result_df = (...)` позволяет актуализировать значения вершин. Эта операция аналогична рассылке сообщений, но со всех вершин и сразу всем.

![Рассылка сообщений](https://habrastorage.org/webt/kv/ix/af/kvixaf4_kjvlmskjocjpvw3vkg8.png)

### Пример работы

![Исходный граф](https://habrastorage.org/webt/yz/gg/jh/yzggjhrflf_8ey-htgoixqbhvng.png)

Граф, представленный на картинке выше содержит две связные компоненты:

1. вершины `{1,2,3,4,5}`,
1. вершины `{6}`.

Приведенный граф можно сгенерировать при помощи следующей функции:

```python
def small_graph():
    """Generates a small graph for tests"""
    vertices = [ (x,) for x in range(1, 7) ]
    vertices_df = (
        spark
            .createDataFrame(vertices)
            .toDF("id")
    )
    edges = [ (1, 2, 1), (1, 3, 5), (2, 3, 1), (3, 4, 1), (4, 5, 1), (6, 6, 1) ]
    edges_df = (
        spark
            .createDataFrame(edges)
            .toDF("src", "dst", "weight")
    )
    return edges_df, vertices_df
```

Подготовка данных:

```python
edges_df, vertices_df = small_graph()
```

Проследим как меняются значения в вершинах:

![Изменение графа](https://habrastorage.org/webt/0y/y6/jb/0yy6jbiqaw2_1r_ouylbkjpbve0.gif)

![Изменение значений](https://habrastorage.org/webt/91/jf/ez/91jfezn1lnwsaxc8anyqwxyron0.gif)

1. Начальное состояние (каждая вершина - свой собственный компонент связности/подграф):

```python
pregel_cc_df = vertices_df.withColumn("value", col("id"))
pregel_cc_df.show()
```

2. Выполним один шаг (superstep) - вершины 2 и 3 напрямую связаны с вершиной 1, поэтому для них значение `value` равно 1:

```python
pregel_cc_df = pregel_cc(edges_df, pregel_cc_df, 1)
pregel_cc_df.show()
```

3. Выполним еще один шаг (superstep) - вершина 4 напрямую связана с вершиной 3, до нее дошло значение `value` равное 1:

```python
pregel_cc_df = pregel_cc(edges_df, pregel_cc_df, 1)
pregel_cc_df.show()
```

4. Выполним еще один шаг (superstep) - вершина 5 напрямую связана с вершиной 4, до нее дошло значение `value` равное 1:

```python
pregel_cc_df = pregel_cc(edges_df, pregel_cc_df, 1)
pregel_cc_df.show()
```

5. Выполним еще один шаг (superstep) - изменений больше нет, алгоритм сошёлся:

```python
pregel_cc_df = pregel_cc(edges_df, pregel_cc_df, 1)
pregel_cc_df.show()
```

Таким образом, получилось два компонента связности: 1 и 6.

### Замечания

Стоит заметить, что нумерация компонентов связности не обязательно непрерывно возрастает на единицу: компоненты связности используют в качестве своих идентификаторов значение идентификатора одной из вершин, входящих в подграф. Этот факт необходимо учитывать, если нужно будет посчитать общее число компонентов связности или выполнить другую аналитику при помощи этой информации.

Реализованный алгоритм останавливается только, когда определенное количество шагов было выполнено. Заинтересованный читатель может реализовать остановку алгоритма по отсутствию новых сообщений самостоятельно. Для этого необходимо добавить новую колонку типа `boolean` под именем `value_changed`, значение которой необходимо установить в `True`, если `F.least("message", "value") != col("value")`. Когда алгоритм сойдется, изменений значения колонки `value` не будет, а следовательно, все строки будут иметь значение `False` в колонке `value`.

## Задача о кратчайшем расстоянии (Single Source Shortest Path)

Решение задачи о кратчайшем расстоянии позволяет найти минимальное расстояние от одной из вершин графа до всех остальных вершин ([Wikipedia](https://ru.wikipedia.org/wiki/%D0%97%D0%B0%D0%B4%D0%B0%D1%87%D0%B0_%D0%BE_%D0%BA%D1%80%D0%B0%D1%82%D1%87%D0%B0%D0%B9%D1%88%D0%B5%D0%BC_%D0%BF%D1%83%D1%82%D0%B8)). В результате в каждой вершине будет храниться сумма всех весов ребер, которые составляют кратчайший путь до текущей вершины от заданной вершины.

### Императивный алгоритм

Алгоритм на Pregel выглядит следующим образом:

```python
class MinDistPregelVertex(PregelVertex[int, Tuple[CCVertex, CCVertex, int], int]):

    def __init__(self, id: int, start_vertex_id: int):
        self.id = id
        self.value = 0 if id == start_vertex_id else sys.maxsize // 2

    def compute(self, messages: List[int]) -> None:
        min_dist_message_value = min(messages)

        if self.value > min_dist_message_value:
            self.value = min_dist_message_value

            for _, target, weight in self.out_edges():
                self.send_message(target.id, self.value + weight)
```

Комментарии к коду:

- `MinDistPregelVertex` наследует `PregelVertex`, где типовые параметры:
    1. `int` - тип значения, т.е. сумма весов ребер до текущей вершины от заданной вершины;
    1. `Tuple[CCVertex, CCVertex, int]` - тип ребра в формате: (исходная вершина, конечная вершина, вес ребра);
    1. `int` - тип сообщения, т.е. сумма весов ребер до соседних вершин.
- в конструктор передаются два параметра:
    1. идентификатор текущей вершины,
    1. идентификатор вершины, от которой необходимо вычислить кратчайшее расстояние до текущей вершины. Возможны два варианта:
        - если текущая вершина является начальной вершиной, то кратчайшее расстояние будет равно нулю,
        - в противном случае начальное значение расстояния считается равным `sys.maxsize // 2` (бесконечности). Такое значение выбрано, чтобы не допустить переполнения целочисленного типа.
- метод `compute` корректирует значение весов ребер кратчайшего расстояния до текущей вершины от заданной:
    1. на вход приходят сообщения, которые были отправлены текущей вершине;
    1. выполняется поиск минимального сообщения;
    1. минимальное сообщение сравнивается с текущим значением - минимальное из двух будет являться значением кратчайшего расстояния до текущей вершины;
    1. если значение в текущей вершине было обновлено, то отправить соседям сообщение состоящее из суммы нового значения и значения веса до соседней вершины.
- реализация остальных методов исключена, т.к. по большей части зависит от окружения.

Процесс вычисления конечного результата так же является итеративным (планировщик выполняет superstep много раз), и программист сам должен управлять сходимостью алгоритма и прервать его, когда полученный результат будет устраивать. В худшем случае (когда граф - это связный список без циклов) количество необходимых шагов (superstep) будет линейно зависеть от диаметра самой большой связной компоненты.

### Алгоритм на Apache Spark

Решение для Apache Spark строится на базе Spark DSL запроса. Для исключения дублирования кода, основной алгоритм вынесен в функцию `pregel_min_dist` (подробные комментарии ниже):

```python
def pregel_min_dist(edges: DataFrame, values: DataFrame, steps: int) -> DataFrame:
    result_df = values
    for i in range(steps):
        result_df = (
            edges.join(result_df, col("src") == col("id"))
                .select(col("dst").alias("id"), F.expr("dist + weight").alias("message"))
                .groupby(col("id")).agg(F.min("message").alias("message"))
                .join(result_df, "id", "right")
                .select("id", F.least("message", "dist").alias("dist"))
        )
        if i % 5 == 0:
            result_df = result_df.checkpoint()

    return result_df
```

Комментарии к коду:

1. на вход приходит 3 параметра:
    1. `edges` - датафрейм с рёбрами графа. В датафрейме обязательно должны быть колонки:
        - `src` - исходная вершина,
        - `dst` - конечная вершина,
        - `weight` - вес ребра.
    1. `values` - датафрейм с вершинами графа. В датафрейме обязательно должны быть колонки:
        - `id` - идентификатор вершины,
        - `dist` - значение минимального расстояния до текущей вершины.
    1. `steps` - количество шагов (superstep), которое необходимо выполнить, чтобы алгоритм сошёлся.
1. каждые пять шагов драйвер будет запускать план на исполнение.

Рассмотрим запрос поближе и укажем на аналогии с кодом на Python для вычисления связных компонент:

1. `edges.join(result_df, col("src") == col("id"))` позволяет соединить датафреймы ребер и вершин, так в результате появятся как минимум пять колонок:
    1. `src` - исходная вершина;
    1. `dst` - конечная вершина;
    1. `id` - идентификатор вершины (равен `src`);
    1. `weight` - вес ребра от вершины `src` до вершины `dst`;
    1. `dist` - текущее значение вершины (`src` и `id`). Сумму этого значения и веса ребра (`weight`) до соседней вершины нужно отправить всем вершинам `dst`.
2. `.select(col("dst").alias("id"), F.expr("dist + weight").alias("message"))` формирует сообщение для каждой вершины. Фактически программа говорит:
    - сейчас у вершины `src` значение `dist`,
    - вес ребра от `src` до `dst` равен `weight`,
    - каждая вершина `dst` должна знать о значении `dist + weight`

    На рисунке ниже можно найти соответствие этой инструкции коду для вычисления связных компонент на Pregel:

![Список сообщений](https://habrastorage.org/webt/hm/d3/iz/hmd3izunpxxsevz-sogapsknjkm.png)

3. `.groupby(col("id")).agg(F.min("message").alias("message"))` позволяет вычислить итоговое сообщение. Вершина `dst` может получить много сообщений от разных `src` вершин, а значит необходимо выбрать из сообщений минимальное значение;

![Получение минимального сообщения](https://habrastorage.org/webt/ib/_u/if/ib_uifm6-s8hmvo3rt87rwt1sfi.png)

4. `.join(result_df, "id", "right")` позволяет соединить итоговые сообщения с датафреймом, в котором находятся текущие значения. Как минимум в итоге получится три колонки:
    1. `id` - идентификатор текущей вершины,
    1. `dist` - текущее значение вершины,
    1. `message` - итоговое сообщение от всех соседей. Нужно проверить меньше ли оно, чем текущее значение `dist`.
5. `.select("id", F.least("message", "dist").alias("dist"))` позволяет вычислить итоговое значение:
    1. `id` - текущая вершина,
    1. `F.least("message", "dist")` - выбрать минимум между сообщением и текущим значением,
    1. `.alias("dist")` - заменить текущее значение `dist` в вершине `id` на вычисленное.

![Вычисление нового значения](https://habrastorage.org/webt/_j/52/gm/_j52gmyap_imj1o-e4_ixo_thg0.png)

6. `result_df = (...)` позволяет актуализировать значения вершин. Эта операция аналогична рассылке сообщений, но со всех вершин и сразу всем.

![Рассылка сообщений](https://habrastorage.org/webt/ci/bt/xh/cibtxhsdgnbgxzmm82-3sbwsot4.png)

### Пример работы

![Исходный граф](https://habrastorage.org/webt/yz/gg/jh/yzggjhrflf_8ey-htgoixqbhvng.png)

Приведенный граф тот же самый, который использовался в алгоритме поиска связных компонент. Минимальное расстояние будет вычисляться относительно вершины 1.

**Подготовка данных**

Функция `get_min_dist_init_df` позволит проинициализировать колонку `dist` по принципу:

1. если идентификатор текущей вершины равен 1, то кратчайшее расстояние равно 0,
1. в противном случае значение вершины равно `sys.maxsize // 2` (бесконечность). Такое значение выбрано, чтобы не выйти за границы целого числа.

```python
def get_min_dist_init_df(values: DataFrame, start: int = 1) -> DataFrame:
    return values.withColumn("dist",
        F.when(
            col("id") == F.lit(start),
            F.lit(0)
        )
        .otherwise(F.lit(sys.maxsize // 2))
    )

edges_df, vertices_df = small_graph()
min_dist_df = get_min_dist_init_df(vertices_df, start=1)
```

Проследим как меняются значения в вершинах:

![Изменение графа](https://habrastorage.org/webt/nh/ai/0x/nhai0xfvbrmwuz87ckxkd1p8doq.gif)

![Изменение значений](https://habrastorage.org/webt/gt/7m/_j/gt7m_joebipzkkdxm6utftkklji.gif)

1. Начальное состояние - значение вершины 1 равно 0, значение остальных - бесконечность:

```python
min_dist_df.show()
```

2. Выполним один шаг (superstep) - вершины 2 и 3 напрямую связаны с вершиной 1, поэтому для них значение `dist` изменяется:

```python
min_dist_df = pregel_min_dist(edges_df, min_dist_df, 1)
min_dist_df.show()
```

3. Выполним еще один шаг (superstep) - вершина 3 связна с вершиной 2, а вершина 4 напрямую связана с вершиной 3, поэтому их значение `dist` изменяется:

```python
min_dist_df = pregel_min_dist(edges_df, min_dist_df, 1)
min_dist_df.show()
```

4. Выполним еще один шаг (superstep) - вершина 5 напрямую связана с вершиной 4, поэтому ее значение `dist` изменяется, а вершина 4 получила сообщение, что значение `dist` вершины 3 изменилось, поэтому значение `dist` вершины 4 так же изменяется:

```python
min_dist_df = pregel_min_dist(edges_df, min_dist_df, 1)
min_dist_df.show()
```

5. Выполним еще один шаг (superstep) - вершина 5 получила новое сообщение от вершины 4, поэтому значение `dist` вершины 5 было уточнено:

```python
min_dist_df = pregel_min_dist(edges_df, min_dist_df, 1)
min_dist_df.show()
```

6. Выполним еще один шаг (superstep) - изменений больше нет, алгоритм сошёлся:

```python
min_dist_df = pregel_min_dist(edges_df, min_dist_df, 1)
min_dist_df.show()
```

Замечания:

- значения `dist` у вершин уточняются на каждом шаге, и в конечном итоге алгоритм сходится,
- вершина 6 не связана с вершиной 1, поэтому минимальное расстояние равно бесконечности.

## Топологическая сортировка (Topological Sort)

Топологическая сортировка ([Wikipedia](https://ru.wikipedia.org/wiki/%D0%A2%D0%BE%D0%BF%D0%BE%D0%BB%D0%BE%D0%B3%D0%B8%D1%87%D0%B5%D1%81%D0%BA%D0%B0%D1%8F_%D1%81%D0%BE%D1%80%D1%82%D0%B8%D1%80%D0%BE%D0%B2%D0%BA%D0%B0)) позволяет отсортировать граф таким образом, что:

- в начале списка будут находиться независимые вершины, на которые никто не ссылается
- далее идут вершины, которые ссылаются на независимые вершины
- далее вершины, которые ссылаются на вершины из предыдущего пункта и т.д.

В результате в каждой вершине будет храниться порядковый номер вершины.

Топологическая сортировка применима только, если в графе нет циклов.

Самый распространенный пример топологической сортировки - это компиляция программ: к моменту компиляции одного модуля, все модули, от которых он зависит, должны быть уже скомпилированы.

### Императивный алгоритм

Основная идея в том, что текущая вершина должна иметь порядок на единицу больше, чем максимальная из вершин, которые ссылаются на нее:

```python
class TopologicalSortPregelVertex(PregelVertex[int, Tuple[CCVertex, CCVertex], int]):

    def __init__(self, id: int):
        self.id = id
        self.value = 1

    def compute(self, messages: List[int]) -> None:
        max_incoming_order = max(messages)

        if self.value < max_incoming_order + 1:
            self.value = max_incoming_order + 1

            for _, target, weight in self.out_edges():
                self.send_message(target.id, self.value)
```

Комментарии к коду:

- `TopologicalSortPregelVertex` наследует `PregelVertex`, где типовые параметры:
    1. `int` - тип значения, поярковый номер вершины;
    1. `Tuple[CCVertex, CCVertex]` - тип ребра в формате: (исходная вершина, конечная вершина);
    1. `int` - тип сообщения, т.е. порядковый номер вершины.
- изначально считается, что каждая вершина независима, поэтому ее порядок равен 1;
- метод `compute` корректирует порядок текущей вершины:
    1. на вход приходят сообщения, которые были отправлены текущей вершине. Сообщения содержат значение порядка вершин;
    1. выполняется поиск максимального порядка среди входящих сообщений;
    1. максимальное значение порядка сравнивается с текущим значением: текущее значение должно быть не меньше, чем максимальный порядок среди всех входящих вершин увеличенный на единицу;
    1. если значение в текущей вершине было обновлено, то отправить соседям сообщение, состоящее из актуального порядка текущей вершины.
- реализация остальных методов исключена, т.к. по большей части зависит от окружения.

Процесс вычисления конечного результата так же является итеративным (планировщик выполняет superstep много раз), и программист сам должен управлять сходимостью алгоритма и прервать его, когда полученный результат будет устраивать. В худшем случае (когда граф - это связный список без циклов) количество необходимых шагов (superstep) будет линейно зависеть от диаметра самой большой связной компоненты.

### Алгоритм на Apache Spark

Решение для Apache Spark строится на базе Spark DSL запроса. Для исключения дублирования кода, основной алгоритм вынесен в функцию `pregel_min_dist` (подробные комментарии ниже):

```python
def pregel_topological_sort(edges: DataFrame, values: DataFrame, steps: int):
    result_df = values
    for i in range(steps):
        result_df = (
            edges.where("src != dst")
                .join(result_df, col("src") == col("id"))
                .select(col("dst").alias("id"), col("ord").alias("message"))
                .groupby(col("id")).agg(F.max("message").alias("message"))
                .join(result_df, "id", "right")
                .select("id", F.greatest(F.expr("message + 1"), "ord").alias("ord"))
        )
        if i % 5 == 0:
            result_df = result_df.checkpoint()

    return result_df
```

Комментарии к коду:

1. на вход приходит 3 параметра:
    1. `edges` - датафрейм с рёбрами графа. В датафрейме обязательно должны быть колонки:
        - `src` - исходная вершина,
        - `dst` - конечная вершина.
    1. `values` - датафрейм с вершинами графа. В датафрейме обязательно должны быть колонки:
        - `id` - идентификатор вершины,
        - `ord` - порядковое значение вершины в графе.
    1. `steps` - количество шагов (superstep), которое необходимо выполнить, чтобы алгоритм сошёлся.
1. каждые пять шагов драйвер будет запускать план на исполнение.

Рассмотрим запрос поближе и укажем на аналогии с кодом на Python для вычисления связных компонент:

1. `edges.where("src != dst")` - исключить ребра, когда вершина ссылается на саму себя. Изначально ожидается, что в графе нет циклов, это лишь дополнительная защита;
1. `edges.join(result_df, col("src") == col("id"))` позволяет соединить датафреймы ребер и вершин, так в результате появятся как минимум четыре колонки:
    1. `src` - исходная вершина;
    1. `dst` - конечная вершина;
    1. `id` - идентификатор вершины (равен `src`);
    1. `ord` - текущее значение вершины (`src` и `id`). Это значение нужно отправить всем вершинам `dst`.
2. `.select(col("dst").alias("id"), col("ord").alias("message"))` формирует сообщение для каждой вершины. Фактически программа говорит, что сообщением является значение `ord`;

    На рисунке ниже можно найти соответствие этой инструкции коду для вычисления связных компонент на Pregel:

![Список сообщений](https://habrastorage.org/webt/qx/ws/00/qxws00wlstgundnswpjljplmwp0.png)

3. `.groupby(col("id")).agg(F.max("message").alias("message"))` позволяет вычислить итоговое сообщение. Вершина `dst` может получить много сообщений от разных `src` вершин, а значит необходимо выбрать из сообщений максимальное значение;

![Получение максимального сообщения](https://habrastorage.org/webt/cr/dc/sz/crdcszlytnt2lcqdeyg5iwbjjre.png)

4. `.join(result_df, "id", "right")` позволяет соединить итоговые сообщения с датафреймом, в котором находятся текущие значения. Как минимум в итоге получится три колонки:
    1. `id` - идентификатор текущей вершины,
    1. `ord` - текущее значение вершины,
    1. `message` - итоговое сообщение от всех соседей. Нужно проверить больше ли текущее значение `ord`, чем входящее сообщение увеличенное на единицу.
5. `.select("id", F.greatest(F.expr("message + 1"), "ord").alias("ord"))` позволяет вычислить итоговое значение:
    1. `id` - текущая вершина,
    1. `F.expr("message + 1")` - входящее сообщение увеличенное на единицу,
    1. `F.greatest(F.expr("message + 1"), "ord").alias("ord"))` - выбрать максимум между сообщением, увеличенным на единицу, и текущим значением,
    1. `.alias("ord")` - заменить текущее значение `ord` в вершине `id` на вычисленное.

![Вычисление нового значения](https://habrastorage.org/webt/de/1k/oi/de1koi9vgikqtjw8jqew1uioyig.png)

6. `result_df = (...)` позволяет актуализировать значения вершин. Эта операция аналогична рассылке сообщений, но со всех вершин и сразу всем.

![Рассылка сообщений](https://habrastorage.org/webt/w7/vy/cc/w7vycco1tgzmjjh-wgc65wu9epg.png)

### Пример работы

![Исходный граф](https://habrastorage.org/webt/yz/gg/jh/yzggjhrflf_8ey-htgoixqbhvng.png)

Приведенный граф тот же самый, который использовался в алгоритме поиска связных компонент.

**Подготовка данных**

Каждая вершина изначально считается независимой, поэтому у них у всех порядковый номер 1 (колонка `ord`).

```python
edges_df, vertices_df = small_graph()
sorted_df = vertices_df.withColumn("ord", F.lit(1))
```

Проследим как меняются значения в вершинах:

![Изменение графа](https://habrastorage.org/webt/f_/go/bh/f_gobh51fjj_zrtvxpczpb2yv-s.gif)

![Изменение значений](https://habrastorage.org/webt/w1/bx/8r/w1bx8rpenx1xxg7lwv0vwdmnc-y.gif)

1. Начальное состояние - значение `ord` всех вершин равно 1:

```python
sorted_df.orderBy("id").show()
```

2. Выполним один шаг (superstep) - все вершины за исключением 1 и 6 имеют входящие ребра, поэтому для них значение `ord` становится равным 2:

```python
sorted_df = pregel_topological_sort(edges_df, sorted_df, 1)
sorted_df.orderBy("ord").show()
```

3. Выполним еще один шаг (superstep) - вершина 3 связна с вершиной 2, поэтому ее значение меняется на 3, т.к ее текущее значение равно 2. Вершины 4 и 5 увеличивают на единицу значение `ord`, т.к. вершина 3 передает вершине 4 значение 2, и вершина 5 получает от вершины 4 значение 2:

```python
sorted_df = pregel_topological_sort(edges_df, sorted_df, 1)
sorted_df.orderBy("ord").show()
```

4. Выполним еще один шаг (superstep) - вершины 1, 2, 3 и 6 больше не получают сообщений, которые были бы больше чем их текущее значение, а вершина 4 получает от вершины 2 значение 3, и вершина 5 так же получает от вершины 4 значение 3, поэтому их значение становится равным 4:

```python
sorted_df = pregel_topological_sort(edges_df, sorted_df, 1)
sorted_df.orderBy("ord").show()
```

5. Выполним еще один шаг (superstep) - только вершина 5 увеличивает свое значение `ord` на единицу, т.к.вершина 4 передала ей значение 4:

```python
sorted_df = pregel_topological_sort(edges_df, sorted_df, 1)
sorted_df.orderBy("ord").show()
```

6. Выполним еще один шаг (superstep) - изменений больше нет, алгоритм сошёлся:

```python
sorted_df = pregel_topological_sort(edges_df, sorted_df, 1)
sorted_df.orderBy("ord").show()
```

Замечания:

- значения `ord` у вершин уточняются на каждом шаге, и в конечном итоге алгоритм сходится,
- алгоритм назначает вершинам их порядковый номер, а реальная сортировка выполняется при помощи `orderBy("ord")`.

## Обобщение алгоритма

Может показаться сложным писать код для каждого случая, но если взглянуть на реализации, то можно найти закономерности.

Поставив все три реализованных алгоритма друг напротив друга можно заметить, что остается неизменным, а какие части меняются:

![Связные компоненты](https://habrastorage.org/webt/rb/m1/et/rbm1etya0l-qns_wmfdwofrajvm.png)

![Кратчайший путь](https://habrastorage.org/webt/qb/en/g1/qbeng1qt5v2nwl9piuwovtqlu8c.png)

![Топологическая сортировка](https://habrastorage.org/webt/s1/ze/v-/s1zev-0l4q9wbjswujsrzofh_48.png)

Основные отличия:

1. на второй строке вычисляется сообщение;
1. на третьей строке выполняется агрегация сообщений;
1. на последней строке выполняется вычисление нового значения вершины.

Дополнительно можно заметить, что для топологической сортировки еще накладывается условие, которое исключает тривиальные циклы (вершина ссылается на саму себя). Датафрейм `edges` передается в качестве параметра, и поэтому программист может сам предварительно наложить это условие перед вызовом функции.

Учитывая найденные отличия можно реализовать обобщенную функцию (подробные комментарии ниже), которая будет вычислять с помощью Pregel любой алгоритм на графах:

```python
def pregel_superstep(
    edges: DataFrame,
    values: DataFrame,
    message: Column,
    combiner: Callable[[Column], Column],
    computer: Column, **columns: Column,
    **columns: Column
) -> DataFrame:
    message_box_df = (
        edges.join(values, col("src") == col("id"))
            .select(col("dst").alias("id"), message.alias("message"))
    )
    accumulator_df = (
        message_box_df
            .groupby(col("id"))
            .agg(combiner(col("message")).alias("message"))
    )
    result_df = (
        accumulator_df
            .join(values, "id", "right")
            .select("id", computer.alias("value"), *columns.values())
    )
    return result_df
```

Комментарии к коду:

1. Функция `pregel_superstep` принимает на вход шесть параметров:
    1. `edges` - датафрейм с ребрами графа,
    1. `values` - датафрейм с вершинами графа и начальными значениями,
    1. `message` - объект типа `Column` выбирает из датафрейма колонку или выражение, которое является присланным сообщением,
    1. `combiner` - лямбда функция (`Column -> Column`), которая агрегирует все входящие сообщения,
    1. `computer` - объект типа `Column` выбирает из датафрейма колонку или выражение, которое будет итоговым значением вершины
    1. `columns` дополнительный набор колонок, которые программист хочет видеть в итоговом датафрейме
1. тело функции разбито на части для упрощения восприятия. Составные части:
    - `message_box_df` - почтовый ящик,
    - `accumulator_df` - агрегация сообщений,
    - `result_df` - датафрейм с вершинами и итоговыми результатами.

### Связные компоненты

При помощи функции можно реализовать алгоритм для поиска связных компонент:

```python
edges_df, vertices_df = small_graph()
cc_df = vertices_df.withColumn("value", col("id"))

for _ in range(5):
    cc_df = pregel_superstep(
        edges=edges_df,
        values=cc_df,
        message=col("value"),
        combiner=F.min,
        computer=F.least(col("value"), col("message"))
    )

cc_df.show()

+---+-----+
| id|value|
+---+-----+
|  1|    1|
|  3|    1|
|  2|    1|
|  4|    1|
|  6|    6|
|  5|    1|
+---+-----+
```

### Кратчайшее расстояние

При помощи функции можно реализовать алгоритм для поиска кратчайшего расстояния:

```python
edges_df, vertices_df = small_graph()
pregel_min_dist_df = get_min_dist_init_df(vertices_df, start=1).withColumnRenamed("dist", "value")

for _ in range(5):
    pregel_min_dist_df = pregel_superstep(
        edges=edges_df,
        values=pregel_min_dist_df,
        message=F.expr("value + weight"),
        combiner=F.min,
        computer=F.least("message", "value")
    )

pregel_min_dist_df.show()

+---+-------------------+
| id|              value|
+---+-------------------+
|  1|                  0|
|  3|                  2|
|  2|                  1|
|  4|                  3|
|  6|4611686018427387903|
|  5|                  4|
+---+-------------------+
```

### Топологическая сортировка

При помощи функции можно реализовать алгоритм топологической сортировки

```python
edges_df, vertices_df = small_graph()
pregel_sort_df = vertices_df.withColumn("value", F.lit(1))

for _ in range(5):
    pregel_sort_df = pregel_superstep(
        edges=edges_df.where("src != dst"),
        values=pregel_sort_df,
        message=col("value"),
        combiner=F.max,
        computer=F.greatest(F.expr("message + 1"), "value")
    )

pregel_sort_df.orderBy("value").show()

+---+-----+
| id|value|
+---+-----+
|  1|    1|
|  6|    1|
|  2|    2|
|  3|    3|
|  4|    4|
|  5|    5|
+---+-----+
```

### PageRank

Алгоритм PageRank ([Wikipedia](https://ru.wikipedia.org/wiki/PageRank)) позволяет определить на сколько авторитетен источник на основе источников, которые ссылаются на него. Изначально был создан для ранжирования страниц при поиске в Google.

Алгоритм PageRank активно работает с исходящей степенью вершины, поэтому необходимо вычислить значение степени для каждой вершины:

```python
out_deg = (
    edges_df
        .groupBy("src")
        .agg(F.count(F.lit(1)).alias("out_deg"))
        .withColumnRenamed("src", "id")
)
```

В формуле PageRank значение out_deg находится в знаменателе, а поэтому необходимо избавиться от нулей и пустых значений:

```python
deg_vert_df = (
    vertices_df
        .join(out_deg, "id", "left")
        .select("id", F.coalesce("out_deg", F.lit(1)).alias("out_deg"))
)
```

Изначально все вершины равны между собой, поэтому для всех устанавливается начальное значение равное 1:

```python
page_rank_result_df = deg_vert_df.withColumn("value", F.lit(1))
```

Алгоритм PageRank никогда не сойдется к какому-то конкретному значению, поэтому очень часто его прерывают после определенного числа шагов, для примера используется 10 шагов:

```python
for i in range(10):
    page_rank_result_df = pregel_superstep(
        edges=edges_df,
        values=page_rank_result_df,
        message=col("value"),
        combiner=F.sum,
        computer=F.expr("(0.15 + 0.85 * nvl(message, value)) / out_deg"),
        out_deg=col("out_deg")
    )
    if i % 5 == 0:
        page_rank_result_df = page_rank_result_df.checkpoint()

page_rank_result_df.show()

+---+--------+-------+
| id|   value|out_deg|
+---+--------+-------+
|  6|1.000000|      1|
|  5|0.626186|      1|
|  4|0.560219|      1|
|  3|0.482611|      1|
|  2|0.260871|      1|
|  1|0.130436|      2|
+---+--------+-------+
```

Результат очень похож на правду:

- на вершину 1 никто не ссылается, поэтому у вершины 1 самое низкое значение;
- на вершину 5 ссылаются все остальные вершины напрямую или через другие вершины, поэтому у вершины 5 самое высокое значение;
- вершина 6 находится вне зоны доступа других вершин, поэтому ее значение осталось равным начальному.

## Финальные мысли по фреймворку Pregel

Фреймворк Pregel продолжает славную традицию парадигмы map-reduce, которая так же была создана в Google. Фреймворк позволяет сегментировать графы любым способом на любое число частей, и он все равно будет работать.

Приведенная реализация алгоритма на Apache Spark полагается на тот факт, что функции, которые агрегируют входящие сообщения являются ассоциативными: (`min(a, min(b, c)) == min(min(a, b), c)`), что позволяет Apache Spark вычислить промежуточные результаты параллельно, а потом собрать все промежуточные результаты для финальной агрегации. Если какой-то из алгоритмов не сможет предоставить ассоциативную функцию для агрегации сообщений, то его невозможно будет реализовать на Apache Spark указанным способом. К счастью, множество задач имеет больше чем одно решение, а поэтому большинство стандартных алгоритмов на графах можно реализовать при помощи Pregel.

Написание алгоритмов для Pregel требует корректировки мышления: программисты с опытом map-reduce смогут быстро разрабатывать свои алгоритмы, остальным потребуется практика.
